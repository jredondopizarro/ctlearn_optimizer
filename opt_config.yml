num_cpus: 12
num_gpus: 2
num_gpus_per_trial: 1
num_cpus_per_trial: 6
optimization_type: 'tree_parzen_estimators'
num_parallel_trials: 2
mode: 'max'
working_directory: 

#Set this option to False if you want to keep the training folders generated by Tensorflow.
#Not mandatory, default = True
remove_training_folders: True
#number of initial random evaluations. Not mandatory, default = 20
n_startup_jobs: 20
#seed. Not mandatory, default = None
random_state: 288
#ctlearn_config file name. Mandatory. There must be a ctlearn config file in the working folder.
ctlearn_config: myconfig.yml
#maximum number of model evaluations. Mandatory
num_max_evals: 40
#Resume optimization run. Valid options: True, False. If True, 'trials.pkl' will be loaded. Not Mandatory, default false
reload_trials: False
#Valid options: True, False, if True, 'checking_file.csv' will be loaded. MandaNot Mandatory, default falsetory
reload_optimization_results: False
#Valid options: True, False. If False the CTLearn model will only be trained. Not Mandatory, default  = False

#Configuration for gaussian_processes based optimization. Not mandatory. 
#Used if optimization_type: 'gaussian_processes'
#Not mandatory, default: None
tree_parzen_estimators_config:
  gamma: 0.25
#Not mandatory, default: None
genetic_algorithm_config:
  #mandatory
  max_generation: 5
  #mandatroy
  population_size: 10
  #not mandatory
  population_decay: 0.95
  keep_top_ratio: 0.2
  selection_bound: 0.4
  crossover_bound: 0.4 
#Not mandatory, default: None
gaussian_processes_config:
  #Valid options: ['GP', 'RF', 'ET', 'GBRT']. Not mandatory, default = 'GP'
  #'GP': Gaussian Processes; 'RF': Random Forests, 'ET': Extended Trees, 
  #'GBRT': Gradient Boosted Trees]. 
  base_estimator: 'GP'
  #Valid options: ['LCB', 'EI', 'PI', 'gp_hedge']. Not mandatory, default = 'gp_hedge'
  #'LCB': lower confidence bound, 'EI': negative expected improvement, 
  #'PI': negative probability of improvement, 'gp_hedge': Probabilistically 
  #choose one of the previous three acquisition functions at every iteration
  acq_function: 'gp_hedge'
  #Valid options: ['auto', 'sampling', 'lbfgs']. Not mandatory, default = 'auto'
  acq_optimizer: 'auto'
  # Controls how much improvement one wants over the previous best values. 
  #Used when the acquisition is either "EI" or "PI". Not mandatory, default = 0.01
  xi: 0.01
  # Controls how much of the variance in the predicted values should be taken 
  #nto account. If set to be very high, then we are favouring exploration over 
  #exploitation and vice versa. Used when the acquisition is "LCB".
  #Not mandatory, default = 1.96
  kappa: 1.96


predict: False
#valid options: ['prediction', 'validation']. If 'prediction', predict must be True. Not Mandatory, default = validation
data_set_to_optimize : 'validation'
#it has to be one metric belonging to metrics_val_to_log or metrics_pred_to_log:
metric_to_optimize : 'auc'
#set validation set metrics to log to the checking_file. Mandatory
#It's required to log at least the metric that is being optimized. _val will be appended to each metric logged
#valid options : ['auc', 'acc', 'acc_gamma', 'acc_proton', 'loss', 'user_defined_metric_val (label)']
metrics_val_to_log: ['auc', 'acc', 'acc_gamma', 'acc_proton', 'loss']
#set prediction set metrics to log to the checking_file. Not mandatory, default = [].
#It's required to log at least the metric that is being optimized. _pred will be appended to each metric logged
#valid options : ['auc', 'acc', 'bacc', 'f1', 'prec', 'rec', 'log_loss', 'user_defined_metric_pred (label)']
metrics_pred_to_log: ['auc', 'acc', 'bacc', 'f1', 'prec', 'rec', 'log_loss']


#set user defined metric to be computed on the validation set. Not mandatory, default = None
#expression has access to ['auc', 'acc', 'acc_gamma', 'acc_proton', 'loss'] (from validation set)
user_defined_metric_val:
  label: 'user_defined_val'
  expression: '(auc + accuracy_gamma)*0.5'
#set user defined metric to be computed on the prediction set, predict must be set to True. Not mandatory, default = None.
#expression has access to ['auc', 'accuracy', 'bacc', 'f1', 'prec', 'rec', 'log_loss'] (from prediction set)
#besides, expression has access to [labels, gamma_classifier_values, predicted_class] and sklearn.metrics
user_defined_metric_pred:
  label: 'user_defined_pred'
  expression: '(auc + f1 + sklearn.metrics.balanced_accuracy_score(labels, predicted_class))*0.5'
#set validation set metrics to log to the checking_file. Mandatory
#It's required to log at least the metric that is being optimized. _val will be appended to each metric logged
#valid options : ['auc', 'acc', 'acc_gamma', 'acc_proton', 'loss', 'user_defined_metric_val (label)']


#Basic config for ctlearn config file. All fields, except prediction_file_list, are mandatory.
Basic_config:
  seed: 1234
  num_training_steps_per_validation: 2500
  num_validations: 15
  #options: ['array', 'single_tel']
  example_type: 'single_tel'
  #options:['cnn_rnn', 'single_tel']
  model: single_tel
  sorting: null
  min_num_tels: 1
  selected_tel_types: ['MST:NectarCam']
  training_file_list: 'data_train.txt'
  #Required if predict = True
  prediction_file_list: 'data_predict.txt'
  batch_size : 64
  model_directory: '/home/jredondo/ctlearn/ctlearn/default_models'
  validation_split: 0.1

#The choice of the hyperparameters' labels is left to the user , but they must
#be the same for all the following subsections
Hyperparameters:
  
  #list of the hyperparameters' labels to log to the checking_file. At least one label is mandatory
  Hyperparameters_to_log: [number_of_layers, layer1_filters,layer2_filters,layer3_filters,
                    layer4_filters,layer1_kernel, layer2_kernel, layer3_kernel, layer4_kernel]

# Dictionary containing (hyperparameter label : list containing hyperparameter CTLearn configuration)
# Must be detailed the configuration of each hyperparameter label used in  Fixed_hyperparameters,
# Dependent_hyperparameters and Hyperparameters_to_optimize subsections. Mandatory
  Config:
    pool_size: ['Model', 'Model Parameters', 'basic', 'conv_block','max_pool','size']
    pool_strides: ['Model', 'Model Parameters', 'basic', 'conv_block','max_pool','strides']
    optimizer_type: ['Training', 'Hyperparameters', 'optimizer']
    base_learning_rate: ['Training', 'Hyperparameters', 'base_learning_rate']
    adam_epsilon: ['Training', 'Hyperparameters', 'adam_epsilon']
    cnn_rnn_dropout: ['Model', 'Model Parameters', 'cnn_rnn', 'dropout_rate']
    layer2_filters: ['Model', 'Model Parameters', 'basic', 'conv_block', 'layers', 1, 'filters']
    layer3_filters: ['Model', 'Model Parameters', 'basic', 'conv_block', 'layers', 2, 'filters']
    layer4_filters: ['Model', 'Model Parameters', 'basic', 'conv_block', 'layers', 3, 'filters']
    layer1_filters: ['Model', 'Model Parameters', 'basic', 'conv_block', 'layers', 0, 'filters']
    layer1_kernel: ['Model', 'Model Parameters', 'basic', 'conv_block', 'layers', 0, 'kernel_size']
    layer2_kernel: ['Model', 'Model Parameters', 'basic', 'conv_block', 'layers', 1, 'kernel_size']
    layer3_kernel: ['Model', 'Model Parameters', 'basic', 'conv_block', 'layers', 2, 'kernel_size']
    layer4_kernel: ['Model', 'Model Parameters', 'basic', 'conv_block', 'layers', 3, 'kernel_size']

# Dictionary containing (hyperparameter label : value) pairs for each fixed hyperparameter.
# Not mandatory, default: None
  Fixed_hyperparameters:
    pool_size: 2
    pool_strides: 2
    optimizer_type: 'Adam'
    base_learning_rate: 5.0e-05
    adam_epsilon: 1.0e-08
    cnn_rnn_dropout: 0.5

# Dictionary containing (hyperparameter label : expression to evaluate) pairs for each dependent
# hyperparameter. Expression has access to all (hyperparameter label: value) pairs of the
# hyperparameters to optimize. Not mandatory, default: None
  Dependent_hyperparameters:

    layer2_filters: '2 * layer1_filters'
    layer3_filters: '4 * layer1_filters'
    layer4_filters: '8 * layer1_filters'

# Mandatory
# Examples are provided
  Hyperparameters_to_optimize:

# Type: hyperparameter distribution (q means integer)
# Valid options for tree_parzen_estimators or random_search optimization: 
#['uniform', 'quniform', 'loguniform',  'qloguniform', 'normal', 'qnormal', 'lognormal', 
#'qlognormal', 'choice', 'conditional']
# Valid options for gaussian_processes optimization: ['uniform', 'quniform', 'loguniform', 
# 'choice']

# range: range of values that the hyperparameter can take
# step: only for q-types (integer types), steps between the values that the 
#hyperparameter can take. Used only for tree_parzen_estimators or random_search 
#optimization

    base_learning_rate:
      type: loguniform
      range: [-5, 0]
    layer1_filters:
      type: 'quniform'
      range: [16, 64]
      step: 1
    layer1_kernel:
      type: 'quniform'
      range: [2, 10]
      step: 1
    layer2_kernel:
      type: 'quniform'
      range: [2, 10]
      step: 1
    layer3_kernel:
      type: 'quniform'
      range: [2, 10]
      step: 1
    layer4_kernel:
      type: 'quniform'
      range: [2, 10]
      step: 1
    optimizer_type:
     type: 'choice'
     range: ['Adadelta', 'Adam', 'RMSProp', 'SGD']
    cnn_rnn_dropout:
     type: 'uniform'
     range: [0,1]

    #example of conditional hyperparameter syntax
    number_of_layers:
     type: 'conditional'
     range:
        - value: 1
          cond_params:
            layer1_kernel:
              type: 'quniform'
              range: [2, 10]
              step: 1
            layer1_filters:
              type: 'quniform'
              range: [16, 64]
              step: 1
        - value: 2
          cond_params:
            layer1_kernel:
              type: 'quniform'
              range: [2, 10]
              step: 1
            layer1_filters:
              type: 'quniform'
              range: [16, 64]
              step: 1
            layer2_kernel:
              type: 'quniform'
              range: [2, 10]
              step: 1
            layer2_filters:
              type: 'quniform'
              range: [16, 128]
              step: 1
